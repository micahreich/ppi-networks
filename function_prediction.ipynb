{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e62c03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import copy\n",
    "from ipynb.fs.full.data_explore import DataExplorer\n",
    "from collections import Counter\n",
    "import pprint\n",
    "from Bio import pairwise2\n",
    "from Bio.SubsMat import MatrixInfo as matlist\n",
    "import operator\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b45aae06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionPrediction():\n",
    "    def __init__(self, organism_name):\n",
    "        # get all the annotation, adjacency list data from the explorer class\n",
    "        self.data_explorer = DataExplorer(organism_name)\n",
    "        \n",
    "    def get_nborhood(self, protein, r, df=\"set\"):\n",
    "        # to get r-neighborhood of protein\n",
    "        if r == 1:\n",
    "            fnl_nborhood = [\n",
    "                nbor for (nbor, score) in self.data_explorer.adj_list[protein]\n",
    "            ]\n",
    "\n",
    "            if df == \"set\":\n",
    "                return set(fnl_nborhood), set(fnl_nborhood)\n",
    "            else:\n",
    "                return fnl_nborhood, fnl_nborhood\n",
    "\n",
    "        accum, last_nborhood = self.get_nborhood(protein, r - 1, df)\n",
    "\n",
    "        fnl_nborhood = []\n",
    "\n",
    "        for protein in last_nborhood:\n",
    "            for (nbor, score) in self.data_explorer.adj_list[protein]:\n",
    "                fnl_nborhood.append(nbor)\n",
    "\n",
    "        if df == \"set\":\n",
    "            return set.union(set(accum), set(fnl_nborhood)), set(fnl_nborhood)\n",
    "        else:\n",
    "            return accum + fnl_nborhood, fnl_nborhood\n",
    "        \n",
    "    def majority_rule(self, protein_id_list, r=1):\n",
    "        \"\"\"\n",
    "        protein_id_list: list of proteins with unknown function you want to predict\n",
    "        r: parameter for radius of neighborhood to consider\n",
    "        \"\"\"\n",
    "        # input: protein list, adjacency list graph representation\n",
    "        # output: cluster assignment for each protein in list based on the majority rule of protein's neighbors\n",
    "        \n",
    "        updated_adj_list = copy.deepcopy(self.data_explorer.adj_list)\n",
    "        # need to remove all proteins in protein_id_list from the adj_list\n",
    "        \n",
    "        # this was me trying to come up with a way to get specific labels for clusters but then realizing\n",
    "        # a paper from 2001 did something that i liked better so i used that below, kept it anyway tho\n",
    "        def tsmallest_cluster_label(protein_id):\n",
    "            print(\"{} has {} neighbors\".format(protein_id, len(self.data_explorer.adj_list[protein_id])))\n",
    "        \n",
    "            tmp = [\n",
    "                self.data_explorer.annotation_list[nbor[0]]\n",
    "                for nbor in self.data_explorer.adj_list[protein_id]\n",
    "            ]\n",
    "\n",
    "            nbor_clusters = []\n",
    "\n",
    "            # need to identify this neighbor's cluster by membership to t^th-smallest cluster, parameterized\n",
    "            for cluster_list in tmp:\n",
    "                # (cluster, size) tuples, sort by size\n",
    "                zipped_list = [\n",
    "                    (cluster_id, self.data_explorer.cluster_sizes[cluster_id])\n",
    "                    for cluster_id in cluster_list\n",
    "                ]\n",
    "\n",
    "                sorted_list = sorted(zipped_list, key=lambda tup: tup[1])\n",
    "\n",
    "                # for each neighbor, we take the t^th smallest cluster that it's a part of, assign that to be\n",
    "                # its identity, then add this cluster to the neighbor cluster list from which we pick the majority\n",
    "                nbor_clusters.append(\n",
    "                    sorted_list[min(t - 1, len(sorted_list) - 1)][0]\n",
    "                )\n",
    "\n",
    "            most_common_nbor = Counter(nbor_clusters).most_common(1)[0][0]\n",
    "            \n",
    "            return most_common_nbor\n",
    "        \n",
    "        def expected_label_weight(protein_id, cluster_id):\n",
    "            # e(v, d, a) = expected number of nodes in a neighborhood of size n(v, d) with function a\n",
    "            # calculated as (# neighbors) * (# nodes labeled cluster_id / # nodes in network)\n",
    "            return len(self.data_explorer.adj_list[protein_id]) * (\n",
    "                self.data_explorer.cluster_sizes[cluster_id] / len(self.data_explorer.adj_list)\n",
    "            )\n",
    "        \n",
    "        def hishigaki_label(protein_id, n=15):\n",
    "            # want to maximize the function:\n",
    "            # |# labeled cluster_id - # expected labeled cluster_id| / # expected labeled cluster_id\n",
    "            # so we are taking the argmax of # labeled cluster_id over all function labels\n",
    "            label_score_dict = {}\n",
    "            \n",
    "            total_nbh, recent_nbh = self.get_nborhood(protein_id, r, df=\"list\")\n",
    "        \n",
    "            clusters = []\n",
    "            for nbor in total_nbh:\n",
    "                if nbor not in protein_id_list:\n",
    "                    clusters += self.data_explorer.annotation_list[nbor]\n",
    "                \n",
    "            cluster_counts = dict(Counter(clusters))\n",
    "            \n",
    "            for cluster in self.data_explorer.cluster_sizes:\n",
    "                e = expected_label_weight(protein_id, cluster)\n",
    "\n",
    "                try:\n",
    "                    f = cluster_counts[cluster]\n",
    "                except KeyError:\n",
    "                    f = 0\n",
    "                \n",
    "                try:\n",
    "                    label_score_dict[cluster] = ((f - e ) ** 2) / e\n",
    "                except:\n",
    "                    label_score_dict[cluster] = 0\n",
    "                        \n",
    "            for cluster in label_score_dict:\n",
    "                if cluster not in cluster_counts:\n",
    "                    label_score_dict[cluster] = 0\n",
    "                else:\n",
    "                    label_score_dict[cluster] = label_score_dict[cluster] * \\\n",
    "                                                1 #(cluster_scores[cluster] / cluster_counts[cluster])\n",
    "            \n",
    "            zipped_dict = [(cluster, label_score_dict[cluster]) for cluster in label_score_dict]\n",
    "            zipped_dict_sorted = sorted(zipped_dict, key = lambda tup: tup[1])\n",
    "\n",
    "            \n",
    "            winners = ([cluster for (cluster, score) in zipped_dict_sorted][::-1])\n",
    "            winners_scores = [label_score_dict[i] for i in winners]\n",
    "            \n",
    "            # winners are returned in reverse sorted order in terms of score, so highest scoring function is at\n",
    "            # index 0 and descends from there\n",
    "            return winners\n",
    "        \n",
    "        cluster_assignments = {}\n",
    "        \n",
    "        for protein in protein_id_list:\n",
    "            cluster_assignments[protein] = hishigaki_label(protein)\n",
    "        \n",
    "        return cluster_assignments\n",
    "            \n",
    "    def functional_flow(self, protein_id_list, t=2):\n",
    "        \"\"\"\n",
    "        protein_id_list: list of proteins with unknown function you want to predict\n",
    "        t: parameter for number of iterations of functional flow; also determines radius of flow from each\n",
    "        unknown protein\n",
    "        \"\"\"\n",
    "        def expected_label_weight(protein_id, cluster_id):\n",
    "            # e(v, d, a) = expected number of nodes in a neighborhood of size n(v, d) with function a\n",
    "            # calculated as (# neighbors) * (# nodes labeled cluster_id / # nodes in network)\n",
    "            return len(self.data_explorer.adj_list[protein_id]) * (\n",
    "                self.data_explorer.cluster_sizes[cluster_id] / len(self.data_explorer.adj_list)\n",
    "            )\n",
    "    \n",
    "        working_nborhood = set([])\n",
    "        for protein_id in protein_id_list:\n",
    "            total_nbh, recent_nbh = self.get_nborhood(protein_id, t, df=\"set\")\n",
    "            working_nborhood.update(total_nbh)\n",
    "    \n",
    "        \n",
    "        print(\"WORKING NBORHOOD SIZE:\", len(working_nborhood))\n",
    "        \n",
    "        # input: protein_id, adjacency list graph representation\n",
    "        # output: cluster_id of protein_id based on the majority rule of protein_id's neighbors\n",
    "        \n",
    "        # idea from Nabieva et al paper:\n",
    "            # Whole-proteome prediction of protein function via graph-theoretic analysis of interaction maps\n",
    "        \n",
    "        # we start by picking some list of proteins that we want to know the function of\n",
    "        # we also start with some known protein functions of nodes in the network\n",
    "        # we fill the known nodes with infinite water at t=0\n",
    "        # for t timesteps, we simulate the flow of water out of the known nodes, along edges, to proteins of\n",
    "        # unknown function, and at t=t, we look at how much water has reached unknown nodes\n",
    "        # (the flow volume between nodes is determined by edge weight)\n",
    "        \n",
    "        resoviors = {0: {}} # fill the nodes at t=0 according to known function or not\n",
    "        # resoviors keep track of resoviors at each timestep\n",
    "        # at a given timestep i, for each node u, we track the amount in the resovior for function a that u has\n",
    "        \n",
    "        print(\"INIT RESOVIORS\")\n",
    "        for protein in self.data_explorer.adj_list:\n",
    "            if protein not in protein_id_list:\n",
    "                function_dict = {}\n",
    "                for cluster in self.data_explorer.cluster_sizes:\n",
    "                    if cluster in self.data_explorer.annotation_list[protein]:\n",
    "                        function_dict[cluster] = 2 ** 100\n",
    "                    else:\n",
    "                        function_dict[cluster] = 0.0\n",
    "                \n",
    "                resoviors[0][protein] = function_dict\n",
    "            else:\n",
    "                function_dict = {}\n",
    "                for cluster in self.data_explorer.cluster_sizes:\n",
    "                    function_dict[cluster] = 0.0\n",
    "                \n",
    "                resoviors[0][protein] = function_dict\n",
    "                        \n",
    "        def edge_capacity(u, v, a, t):\n",
    "            cluster_size = self.data_explorer.cluster_sizes[a]\n",
    "            \n",
    "            if t == 0: return 0\n",
    "            if resoviors[t - 1][u][a] <= resoviors[t - 1][v][a]:\n",
    "                return 0\n",
    "            else:\n",
    "                uv_weight = 0\n",
    "                for (protein, score) in self.data_explorer.adj_list[u]:\n",
    "                    if protein == v:\n",
    "                        uv_weight = score\n",
    "                \n",
    "                u_weights = sum([score for (_, score) in self.data_explorer.adj_list[u]])\n",
    "                \n",
    "                return min(\n",
    "                    uv_weight,\n",
    "                    (uv_weight / u_weights)\n",
    "                ) # / cluster_size\n",
    "       \n",
    "        # initialize the label scoring dictionary\n",
    "        label_score_dict = {}\n",
    "        for protein in protein_id_list:\n",
    "            label_score_dict[protein] = {}\n",
    "            for cluster in self.data_explorer.cluster_sizes:\n",
    "                label_score_dict[protein][cluster] = 0.0\n",
    "        \n",
    "        # build the resoviors\n",
    "        print(\"----- FF SIMULATION -----\")\n",
    "        \n",
    "        for i in range(1, t + 1):\n",
    "            print(\"ITER: {} / {}\".format(i, t))\n",
    "            resovior_i = copy.deepcopy(resoviors[i - 1])\n",
    "            for protein in working_nborhood:\n",
    "                for cluster in self.data_explorer.cluster_sizes:\n",
    "                    test_val = sum(\n",
    "                        [ edge_capacity(nbor, protein, cluster, i) - edge_capacity(protein, nbor, cluster, i)\n",
    "                          for (nbor, score) in self.data_explorer.adj_list[protein] ]\n",
    "                    )\n",
    "                    resovior_i[protein][cluster] = resoviors[i - 1][protein][cluster] + test_val\n",
    "                    \n",
    "            resoviors[i] = resovior_i\n",
    "            \n",
    "            for protein in protein_id_list:\n",
    "                for cluster in self.data_explorer.cluster_sizes:\n",
    "                    for (nbor, score) in self.data_explorer.adj_list[protein]:\n",
    "                        label_score_dict[protein][cluster] += edge_capacity(nbor, protein, cluster, i)\n",
    "        \n",
    "        # determine winning clusters for unknown proteins\n",
    "        cluster_assignments_dict = {}\n",
    "        \n",
    "        for protein in protein_id_list:\n",
    "            cluster_assignments_dict[protein] = []\n",
    "            for cluster in label_score_dict[protein]:\n",
    "                if label_score_dict[protein][cluster] > 0: # add all non-zero flow scores\n",
    "                    cluster_assignments_dict[protein].append((cluster, label_score_dict[protein][cluster]))\n",
    "        \n",
    "        cluster_assignments = {}\n",
    "        \n",
    "        for protein in cluster_assignments_dict:\n",
    "            score_list = cluster_assignments_dict[protein]\n",
    "            sorted_score_list = [cluster for (cluster, score) in sorted(score_list, key = lambda tup: tup[1])]\n",
    "            cluster_assignments[protein] = sorted_score_list[::-1]\n",
    "        \n",
    "        # winners are returned in reverse sorted order in terms of score, so highest scoring function is at\n",
    "        # index 0 and descends from there\n",
    "        return cluster_assignments\n",
    "    \n",
    "    def alignment_approach(self, protein_id_list, r=1):\n",
    "        scoring_matrix = matlist.blosum62\n",
    "        \n",
    "        def gap_function(x, y):  # x is gap position in seq, y is gap length\n",
    "            if y == 0:  # no gap\n",
    "                return 0\n",
    "            elif y == 1:  # gap open penalty\n",
    "                return -2\n",
    "            return - (2 + y/4.0 + log(y)/2.0)\n",
    "        \n",
    "        def single_alignment(protein_id, r=r):\n",
    "            if protein_id not in self.data_explorer.sequences:\n",
    "                raise Exception(\"No sequence available\")\n",
    "            \n",
    "            total_nbh, recent_nbh = self.get_nborhood(protein_id, r, df=\"set\")\n",
    "            score_list = {}\n",
    "            \n",
    "            print(\"WORKING NBORHOOD SIZE:\", len(total_nbh))\n",
    "            \n",
    "            # perform global alignment of all neighbors against alignment of protein, take whichever ones score best\n",
    "            i = 0\n",
    "            for nbor in recent_nbh:\n",
    "                # print(i)\n",
    "                i += 1\n",
    "                if nbor not in protein_id_list:\n",
    "                    try:\n",
    "                        alignments = pairwise2.align.globalds(self.data_explorer.sequences[protein_id], \n",
    "                                                              self.data_explorer.sequences[nbor], \n",
    "                                                              scoring_matrix,\n",
    "                                                              -2, -1\n",
    "                                                             )\n",
    "                        score_list[nbor] = max([algn.score for algn in alignments])\n",
    "                    except KeyError:\n",
    "                        # couldnt find nbor sequence, sad.\n",
    "                        pass\n",
    "            print(score_list)\n",
    "            winner = max(score_list.items(), key=operator.itemgetter(1))[0]\n",
    "            print(winner)\n",
    "            return self.data_explorer.annotation_list[winner]\n",
    "        \n",
    "        cluster_assignments = {}\n",
    "        \n",
    "        for protein in protein_id_list:\n",
    "            cluster_assignments[protein] = single_alignment(protein)\n",
    "        return cluster_assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421cdbf0",
   "metadata": {},
   "source": [
    "### Usage of ``FunctionPrediction`` class:\n",
    "To create a new instance of the class, create a new variable with the organism name (same as organism name used to instantiate the DataExplorer class; will be the name of the folder which contains the PPI info):\n",
    "```\n",
    "function_predictor = FunctionPrediction(\"organism_name\")\n",
    "```\n",
    "\n",
    "To perform the majority approach with a radius `r` and list of proteins to predict `p`, run the function:\n",
    "```\n",
    "majority_approach_results = function_predictor.majority_rule(p, r=r)\n",
    "```\n",
    "\n",
    "To perform functional flow for `t` iterations and list of proteins to predict `p`, run the function:\n",
    "```\n",
    "functional_flow_results = function_predictor.functional_flow(p, t=t)\n",
    "```\n",
    "To perform sequence alignment approach with a radius `r` and list of proteins to predict `p`, run the function:\n",
    "```\n",
    "alignment_approach_results = function_predictor.alignment_approach(p, r=r)\n",
    "```\n",
    "\n",
    "In all methods, the function returns a dictionary of the form:\n",
    "```\n",
    "{protein_id: [list of functions (by name) scored and sorted in descending order]}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd0e54e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_predictor = FunctionPrediction(\"small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86b58169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "636\n"
     ]
    }
   ],
   "source": [
    "# p = [\"1110504.MAGb_1510\"]\n",
    "# functional_flow_results = function_predictor.functional_flow(p, t=3)\n",
    "# sequence_results = function_predictor.alignment_approach(p)\n",
    "print(\n",
    "    len(function_predictor.data_explorer.adj_list)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbc17ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
